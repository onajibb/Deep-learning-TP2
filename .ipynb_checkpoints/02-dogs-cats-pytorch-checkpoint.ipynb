{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c3c813b-abc1-4521-bf2a-1a195ad2da6e",
   "metadata": {},
   "source": [
    "# Dogs vs. cats (PyTorch)\n",
    "\n",
    "In this activity, the goal is to distinguish the animals pictured in these images between cats and dogs.\n",
    "\n",
    "The 2,000 images used in this kata are excerpted from the [Dogs vs. Cats](https://www.kaggle.com/c/dogs-vs-cats/) dataset available on Kaggle, which contains 25,000 images. Here, we use a subset of the full dataset to decrease training time for educational purposes.\n",
    "\n",
    "![Woof Meow](woof_meow.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b44e152-6484-40f9-8713-5a411ee06b9a",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504abbcf-d973-4546-a1a3-b380a187d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Python Imaging Library is needed to display images \n",
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efffbe0d-e28b-490c-9453-b0502e3ecf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base packages\n",
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4bfc54-f2cb-4548-85dd-dcc80d3d0fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup plots\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 10, 8\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "# sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1781adad-7e2b-4b7d-adde-88cd48dbf2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ML packages (edit this list if needed)\n",
    "import torch\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6122fc70-6622-4fea-acb1-5613d2bdc825",
   "metadata": {},
   "source": [
    "## Utilify functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f390a96-9eef-474a-8892-773efc227875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(pred_func, X, y, figure=None):\n",
    "    \"\"\"Plot a decision boundary\"\"\"\n",
    "\n",
    "    if figure is None:  # If no figure is given, create a new one\n",
    "        plt.figure()\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole grid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd7f3df-9e6d-4725-8c93-2848aa7ea877",
   "metadata": {},
   "source": [
    "## Step 1: Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a6f0b2-732f-4bcc-bcb8-da9afb57f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading our example data, a .zip of 2,000 JPG pictures, and extracting it locally in `/tmp`\n",
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
    "    -O /tmp/cats_and_dogs_filtered.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dcc7db-feb3-4e55-8f64-cc9ef9239ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting zip file to the base directory `/tmp/cats_and_dogs_filtered`\n",
    "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/tmp')\n",
    "zip_ref.close()\n",
    "\n",
    "base_dir = '/tmp/cats_and_dogs_filtered'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c31ab0-1da4-4bb6-ab81-0ddd21f49c79",
   "metadata": {},
   "source": [
    "## Step 2: Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300c2b5e-ddab-4925-a4ee-4fd09405e4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training cat images: {len(os.listdir(train_cats_dir))}')\n",
    "print(f'Training dog images: {len(os.listdir(train_dogs_dir))}')\n",
    "print(f'Validation cat images: {len(os.listdir(validation_cats_dir))}')\n",
    "print(f'Validation dog images: {len(os.listdir(validation_dogs_dir))}')\n",
    "\n",
    "# Display some images files for cats and dogs\n",
    "train_cat_fnames = os.listdir(train_cats_dir)\n",
    "print(train_cat_fnames[:10])\n",
    "train_dog_fnames = os.listdir(train_dogs_dir)\n",
    "train_dog_fnames.sort()\n",
    "print(train_dog_fnames[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a1c8c1-618b-4db1-a96a-d9c9aab9fc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for our graph; we'll output images in a 4x4 configuration\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "\n",
    "# Index for iterating over images\n",
    "pic_index = 0\n",
    "\n",
    "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(ncols * 4, nrows * 4)\n",
    "\n",
    "pic_index += 8\n",
    "next_cat_pix = [os.path.join(train_cats_dir, fname) \n",
    "                for fname in train_cat_fnames[pic_index-8:pic_index]]\n",
    "next_dog_pix = [os.path.join(train_dogs_dir, fname) \n",
    "                for fname in train_dog_fnames[pic_index-8:pic_index]]\n",
    "\n",
    "for i, img_path in enumerate(next_cat_pix+next_dog_pix):\n",
    "    # Set up subplot; subplot indices start at 1\n",
    "    sp = plt.subplot(nrows, ncols, i + 1)\n",
    "    img = mpimg.imread(img_path)\n",
    "    plt.imshow(img)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9a0050-df51-4b9c-9721-b805b6285dbb",
   "metadata": {},
   "source": [
    "## Step 3: Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554da00f-8efb-4d24-9d14-7e736c1d764d",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Prepare the data by resizing all images to 150x150 and normalizing them.\n",
    "\n",
    "Create a `DataLoader` for training data and another one for validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c150cf-710c-475a-96b2-cbbba0e3b0be",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e081d15fb1b2d02e35785f083d56769f",
     "grade": false,
     "grade_id": "cell-be81bb30680e54b6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde8645d-0123-4518-a256-9d67187ed81a",
   "metadata": {},
   "source": [
    "## Step 4: Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0910a855-9c27-407b-beaf-15f38237b4a5",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Create a CNN model able to be trained on 150x150x3 images. Instantiate it into the `net` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad69cf19-eb96-4a4c-9e17-f26aafb65749",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9913fae902799d5b5e1962a454b2cc67",
     "grade": false,
     "grade_id": "cell-eb86033963b0a831",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86b3a64-3d8f-4a1b-ae0e-bd7ca8d0fbe7",
   "metadata": {},
   "source": [
    "## Step 5: Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c98b2b7-593c-4bc3-b4e8-befd768f6a09",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Compute accuracy on validation data. Store it into the `val_acc` variable.\n",
    "\n",
    "Validation accuracy should be > 70%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af8e678-4e94-4a76-8b6a-80cdd89bdcb5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad40e0b975bbdd0a9f2876744fd7ce75",
     "grade": false,
     "grade_id": "cell-1348d633ce6cead0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3802dd47-4d11-4dfa-8903-61ed56cfa712",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Validation accuracy: {val_acc * 100}%')\n",
    "\n",
    "# Assert final accuracy\n",
    "assert val_acc > 0.70"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
