{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76bd5de9-2239-4bd9-8304-a93cc1718701",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "257136f7660496aa1a579114822b833f",
     "grade": false,
     "grade_id": "cell-f4b1cb35e89ed84f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Dogs vs. cats (Keras)\n",
    "\n",
    "In this activity, the goal is to distinguish the animals pictured in these images between cats and dogs.\n",
    "\n",
    "The 2,000 images used in this kata are excerpted from the [Dogs vs. Cats](https://www.kaggle.com/c/dogs-vs-cats/) dataset available on Kaggle, which contains 25,000 images. Here, we use a subset of the full dataset to decrease training time for educational purposes.\n",
    "\n",
    "![Woof Meow](woof_meow.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171dfe34-6c0d-42c1-bdcb-0d4af7b6b576",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "661cb0949c72603e8c127a3fc46d926d",
     "grade": false,
     "grade_id": "cell-1b87c3cdd6e4e6f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec9a6f0-d48f-4ab2-8968-45e48308d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Python Imaging Library is needed to display images \n",
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32f62953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/onajib/miniconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72d2f0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-29 16:03:26.074016: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-29 16:03:26.177020: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-29 16:03:26.177035: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-29 16:03:26.811300: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-29 16:03:26.811346: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-29 16:03:26.811353: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-11-29 16:03:27.381840: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-29 16:03:27.382264: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-29 16:03:27.382356: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-29 16:03:27.382434: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-29 16:03:27.382504: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-11-29 16:03:27.382571: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-11-29 16:03:27.382644: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-29 16:03:27.382715: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-29 16:03:27.382782: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-11-29 16:03:27.382798: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939b9f5d-f247-4d40-b7b3-b3b8481926c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base packages\n",
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be52f21a-f959-425a-a502-fc2ed1f3e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup plots\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 10, 8\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ac84a2-2eb6-4e78-8087-f250bfee41b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ML packages (edit this list if needed)\n",
    "import tensorflow as tf\n",
    "print(f'TensorFlow version: {tf.__version__}')\n",
    "print(f'Keras version: {tf.keras.__version__}')\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, array_to_img, load_img\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Input, Dropout\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a4cc4b-78e3-45d9-9718-651fc5147564",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e3c8acd312e200caaf3c278191014fd1",
     "grade": false,
     "grade_id": "cell-de441609300c1508",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ef8210-9a65-43e7-82b2-bf1db43fc77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_acc(history):\n",
    "    \"\"\"Plot training and (optionally) validation loss and accuracy\"\"\"\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(epochs, loss, '.--', label='Training loss')\n",
    "    final_loss = loss[-1]\n",
    "    title = 'Training loss: {:.4f}'.format(final_loss)\n",
    "    plt.ylabel('Loss')\n",
    "    if 'val_loss' in history.history:\n",
    "        val_loss = history.history['val_loss']\n",
    "        plt.plot(epochs, val_loss, 'o-', label='Validation loss')\n",
    "        final_val_loss = val_loss[-1]\n",
    "        title += ', Validation loss: {:.4f}'.format(final_val_loss)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "    acc = history.history['accuracy']\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(epochs, acc, '.--', label='Training acc')\n",
    "    final_acc = acc[-1]\n",
    "    title = 'Training accuracy: {:.2f}%'.format(final_acc * 100)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    if 'val_accuracy' in history.history:\n",
    "        val_acc = history.history['val_accuracy']\n",
    "        plt.plot(epochs, val_acc, 'o-', label='Validation acc')\n",
    "        final_val_acc = val_acc[-1]\n",
    "        title += ', Validation accuracy: {:.2f}%'.format(final_val_acc * 100)\n",
    "    plt.title(title)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6ba18f-1209-41ae-851b-b1a157dd613d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9b1fadcb507fda764da83c2200bf480",
     "grade": false,
     "grade_id": "cell-f94dcdaa420dcb0a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Step 1: Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a29f836-9630-49e1-a6d4-69be89aed904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading our example data, a .zip of 2,000 JPG pictures, and extracting it locally in `/tmp`\n",
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
    "    -O /tmp/cats_and_dogs_filtered.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a315e632-e37d-4b3b-ba1d-84c95b93a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting zip file to the base directory `/tmp/cats_and_dogs_filtered`\n",
    "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/tmp')\n",
    "zip_ref.close()\n",
    "\n",
    "base_dir = '/tmp/cats_and_dogs_filtered'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1398cb06-e1a6-4b6a-8288-5b6d85e2d148",
   "metadata": {},
   "source": [
    "## Step 2: Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95048d5-9cd0-42ef-abf9-0989d83241f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training cat images: {len(os.listdir(train_cats_dir))}')\n",
    "print(f'Training dog images: {len(os.listdir(train_dogs_dir))}')\n",
    "print(f'Validation cat images: {len(os.listdir(validation_cats_dir))}')\n",
    "print(f'Validation dog images: {len(os.listdir(validation_dogs_dir))}')\n",
    "\n",
    "# Display some images files for cats and dogs\n",
    "train_cat_fnames = os.listdir(train_cats_dir)\n",
    "print(train_cat_fnames[:10])\n",
    "train_dog_fnames = os.listdir(train_dogs_dir)\n",
    "train_dog_fnames.sort()\n",
    "print(train_dog_fnames[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd2cb22-a244-4997-91a8-dd344d760966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for our graph; we'll output images in a 4x4 configuration\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "\n",
    "# Index for iterating over images\n",
    "pic_index = 0\n",
    "\n",
    "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(ncols * 4, nrows * 4)\n",
    "pic_index += 8\n",
    "next_cat_pix = [os.path.join(train_cats_dir, fname) \n",
    "                for fname in train_cat_fnames[pic_index-8:pic_index]]\n",
    "next_dog_pix = [os.path.join(train_dogs_dir, fname) \n",
    "                for fname in train_dog_fnames[pic_index-8:pic_index]]\n",
    "\n",
    "for i, img_path in enumerate(next_cat_pix+next_dog_pix):\n",
    "    # Set up subplot; subplot indices start at 1\n",
    "    sp = plt.subplot(nrows, ncols, i + 1)\n",
    "    sp.axis('Off') # Don't show axes (or gridlines)\n",
    "    img = mpimg.imread(img_path)\n",
    "    plt.imshow(img)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44ec2d6-8fd1-4e44-b190-7b998f66a2bb",
   "metadata": {},
   "source": [
    "## Step 3 : training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b7e01c-2e17-4c34-9872-e9309cb8e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Flow training images in batches of 20 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,  # This is the source directory for training images\n",
    "        target_size=(150, 150),  # All images will be resized to 150x150\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "# Flow validation images in batches of 20 using test_datagen generator\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b3c67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can use image_dataset_from_directory to fit the memory\n",
    "X_train = train_generator[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944bacf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69188c72-3b5d-43fc-849a-6357a6eba77b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "504a4b411164ed2c6ee4e6499eb50e43",
     "grade": false,
     "grade_id": "cell-588cc15d5eda0534",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Question\n",
    "\n",
    "Create a CNN model able to be trained on 150x150x3 images. Show its summary with the `model.summary()`method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0216dc3-5ce2-4529-a3df-f900cb8ab049",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ab0a01267db88ac1ff400435292a97e",
     "grade": false,
     "grade_id": "cell-ecadf8c298a6826f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# https://www.tensorflow.org/tutorials/load_data/images\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "#class MyModel(tf.keras.Model):\n",
    "\n",
    "#  def __init__(self):\n",
    "#    super().__init__()\n",
    "#    self.rescale = tf.keras.layers.Rescaling(1./255),\n",
    "#    self.conv1 = tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "#    self.pool1 = tf.keras.layers.MaxPooling2D(),\n",
    "#    self.conv2 = tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "#    self.pool2 = tf.keras.layers.MaxPooling2D(),\n",
    "#    self.conv3 = tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "#    self.pool3 = tf.keras.layers.MaxPooling2D(),\n",
    "#    self.flar = tf.keras.layers.Flatten(),\n",
    "#    self.activation = tf.keras.layers.Dense(128, activation='relu'),\n",
    "#    self.predict = tf.keras.layers.Dense(2)\n",
    "\n",
    "\n",
    "#  def call(self, inputs, training=False):\n",
    "\n",
    "#    h1 = self.conv1(inputs)\n",
    "#    h2 = self.pool1(h1)\n",
    "#    h3 = self.conv2(h2)\n",
    "#    h4 = self.pool2(h3)\n",
    "#    h5 = self.conv3(h4)\n",
    "#    h6 = self.pool3(h5)\n",
    "#    h7 = self.conv4(h6)\n",
    "#    flat = self.flat(h7)\n",
    "#    activ = self.activation(flat)\n",
    "\n",
    "#    return self.predict(activ)\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc9a864-081a-403a-b6f8-f71431a4867c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "547478d08fee3f12d2b2570b90b7a850",
     "grade": false,
     "grade_id": "cell-06ff42510fa8937b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Question\n",
    "\n",
    "Compile and train your model to reach a validation accuracy > 70%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1c2a54-1e65-4dab-b03a-2c51283e55df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the model\n",
    "model.compile('adam',\n",
    "              'binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\" data augmentation, batch norm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f76551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
    "      epochs=15,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50,  # 1000 images = batch_size * steps\n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891fdbf1-a24b-4140-95ae-5c707ae97fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training history\n",
    "plot_loss_acc(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf59c2d3-f8a1-4d47-86b3-300018d89236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve final validation accuracy\n",
    "val_acc = history.history['val_accuracy'][-1]\n",
    "\n",
    "# Assert final accuracy\n",
    "assert val_acc > 0.68"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5474a6d-5f00-454a-9f06-69c085ca261c",
   "metadata": {},
   "source": [
    "## Step 4: Preventing overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3eb089-8219-4f79-84ea-a64279f597b2",
   "metadata": {},
   "source": [
    "### Adding data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e7e12c-76d8-4494-9de0-5a65bbdeaf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "img_path = os.path.join(train_cats_dir, train_cat_fnames[2])\n",
    "img = load_img(img_path, target_size=(150, 150))  # this is a PIL image\n",
    "x = img_to_array(img)  # Numpy array with shape (150, 150, 3)\n",
    "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n",
    "\n",
    "# The .flow() command below generates batches of randomly transformed images\n",
    "# It will loop indefinitely, so we need to `break` the loop at some point!\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1):\n",
    "    plt.figure(i, figsize=(4, 4))\n",
    "    imgplot = plt.imshow(array_to_img(batch[0]))\n",
    "    i += 1\n",
    "    if i % 5 == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069490fe-887a-49fb-9acf-69325fa98c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding rescale, rotation_range, width_shift_range, height_shift_range,\n",
    "# shear_range, zoom_range, and horizontal flip to our ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,)\n",
    "\n",
    "# Note that the validation data should not be augmented!\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Flow training images in batches of 32 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,  # This is the source directory for training images\n",
    "        target_size=(150, 150),  # All images will be resized to 150x150\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "# Flow validation images in batches of 32 using test_datagen generator\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf84a69-3a6a-498a-934f-6076c318793f",
   "metadata": {},
   "source": [
    "### Adding Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e73fdb1-5840-48f2-8be8-87dee9c2aa96",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Create a new model with a `Dropout` layer to your model just before the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc260c9-3d03-4af6-b69f-6124b85ded36",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f307ed7d811f6b42a4965352dd0b149d",
     "grade": false,
     "grade_id": "cell-e654287cb149488b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd95ac2-10bf-4c3a-b434-a3144870002f",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Compile and train your model to reach a validation accuracy > 73%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7742574-6fd7-4eb4-957c-12419c902230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the model\n",
    "model.compile('adam',\n",
    "              'binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50,  # 1000 images = batch_size * steps\n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c2e08a-091a-40d0-8003-ed2df96f3e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training history\n",
    "plot_loss_acc(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697adabd-9437-4d79-9b5a-3be51b5372ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve final validation accuracy\n",
    "val_acc = history.history['val_accuracy'][-1]\n",
    "\n",
    "# Assert final accuracy\n",
    "assert val_acc > 0.73"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9d3750-06e8-4687-9091-20f6cd3e0096",
   "metadata": {},
   "source": [
    "## Step 5: Using a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41e66f4-72ad-4c5c-8b37-9fd7fbb3d47e",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Implement feature extraction, using the convolution base of VGG16 in the `conv_base`variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3c3d50-05f4-4a7a-a8d3-da0cf5294cb1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d463a565e213f639ef9e17c70770ae07",
     "grade": false,
     "grade_id": "cell-89a534cc3548674e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a54fcea-5725-4784-ac77-d2308a43ad55",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Add a Dense classifier on top of the convolutional base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55326996-43e9-417e-8c20-e1ca0e646367",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fef7fdd21cdf19d6050448db5cafea33",
     "grade": false,
     "grade_id": "cell-bee6f54ac3f7e53e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d9976e-4443-4aff-acf5-21c48c6aed00",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Compile and train your model to reach a validation accuracy > 87%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fbcbf9-555f-4376-9b78-533a9a589dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the model\n",
    "\n",
    "model.compile('adam',\n",
    "              'binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
    "      epochs=15,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50,  # 1000 images = batch_size * steps\n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c530ae-d8cf-4167-84f1-60f9cbf4489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training history\n",
    "plot_loss_acc(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120b8d0c-8ae3-4cc4-9176-adc585c05f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve final validation accuracy\n",
    "val_acc = history.history['val_accuracy'][-1]\n",
    "\n",
    "# Assert final accuracy\n",
    "assert val_acc > 0.87"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e3bdec39e15a438669257286c8b8f3b87f2629f073c7f82c439886f14058a852"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
